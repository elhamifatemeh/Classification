{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfhjAt7iDKuR"
      },
      "source": [
        "**Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgiTSSuZCO7v"
      },
      "outputs": [],
      "source": [
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install hazm\n",
        "!pip install -q clean-text[gpl]\n",
        "!pip install -q hazm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYXWhomaDX6P"
      },
      "source": [
        "**Import required packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qqOURsinDYVH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from hazm import *\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import numpy as np\n",
        "import re\n",
        "from string import punctuation\n",
        "from cleantext import clean\n",
        "import hazm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQRPeiw3DYuc"
      },
      "source": [
        "# Preparing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrw0gMv-DoC1"
      },
      "source": [
        "**Convert txt to csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "a5PdK3UxDZFs"
      },
      "outputs": [],
      "source": [
        "file = open('Hamshahri-Corpus.txt', 'r')\n",
        "file2 = open('Corpus.txt', 'w')\n",
        "counter = 0\n",
        "for line in file.read().splitlines():\n",
        "  if \".Cat\\t\" in line and counter == 0:\n",
        "    counter += 1\n",
        "    file2.write(line.replace(\".Cat\\t\", \"\"))\n",
        "    file2.write(',,,,,,,,,,')\n",
        "\n",
        "  elif \".Cat\\t\" in line and counter != 0:\n",
        "    file2.write(',,,,,,,,,,')\n",
        "    file2.write('\\n')\n",
        "    file2.write(line.replace(\".Cat\\t\", \"\"))\n",
        "    file2.write(',,,,,,,,,,')\n",
        "\n",
        "  elif re.search('^[\\u0600-\\u06FF\\s]', line):\n",
        "  #elif re.search('[^ \\u0622\\u0627\\u0628\\u067E\\u062A-\\u062C\\u0686\\u062D-\\u0632\\u0698\\u0633'\n",
        "  #      '-\\u063A\\u0641\\u0642\\u06A9\\u06AF\\u0644-\\u0648\\u06CC\\u200c]', line):\n",
        "    file2.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Corpus.txt\", sep=',,,,,,,,,,', lineterminator='\\n', index_col=-1, header=None)\n",
        "df.columns = ['Cat', 'Text']\n",
        "\n",
        "# store dataframe into csv file\n",
        "df.to_csv('Corpus.csv', index=None)\n",
        "Corpus = pd.read_csv('Corpus.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "IsHZ5zDVkk2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfGPvkKvDv7E"
      },
      "source": [
        "**Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=[]\n",
        "with open('PersianStopWords.txt') as f:\n",
        "    lines = f.read().splitlines()\n",
        "    for line in lines:\n",
        "      line = re.sub(\"\\n\", \"\", line)\n",
        "      stop_words.append(line)"
      ],
      "metadata": {
        "id": "3k8pMkgRQKkU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cleantext import clean\n",
        "import hazm\n",
        "def cleaning(text):\n",
        "\n",
        "    if type(text) == str:\n",
        "      text = text.strip()\n",
        "    \n",
        "    # regular cleaning\n",
        "    text = clean(text,\n",
        "        fix_unicode=True,\n",
        "        to_ascii=False,\n",
        "        lower=False,\n",
        "        no_line_breaks=True,\n",
        "        no_urls=True,\n",
        "        no_emails=True,\n",
        "        no_phone_numbers=True,\n",
        "        no_numbers=False,\n",
        "        no_digits=False,\n",
        "        no_currency_symbols=True,\n",
        "        no_punct=False,\n",
        "        replace_with_url=\" \",\n",
        "        replace_with_email=\" \",\n",
        "        replace_with_phone_number=\" \",\n",
        "        replace_with_number=\" \",\n",
        "        replace_with_digit=\" \",\n",
        "        replace_with_currency_symbol=\" \",\n",
        "    )\n",
        "    normalizer = hazm.Normalizer()\n",
        "    text = normalizer.normalize(text)\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words) # remove stopwors from text\n",
        "    \n",
        "    text = re.sub(\n",
        "        '[^ \\u0622\\u0627\\u0628\\u067E\\u062A-\\u062C\\u0686\\u062D-\\u0632\\u0698\\u0633'\n",
        "        '-\\u063A\\u0641\\u0642\\u06A9\\u06AF\\u0644-\\u0648\\u06CC\\u200c]',\n",
        "        \"\", text)\n",
        "    \n",
        "    # removing extra spaces\n",
        "    text = re.sub(\"\\s+\", \" \", text)\n",
        "\n",
        "    text = bytes(text, 'utf-8').decode('utf-8', 'ignore')\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "ydBaCKj1QND7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Corpus['Text'] = Corpus['Text'].apply(cleaning)"
      ],
      "metadata": {
        "id": "pkSh_xoaQNq7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Corpus['Cat'] = [word.lower() for word in Corpus['Cat']]"
      ],
      "metadata": {
        "id": "YlwnCY54QPg7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing values**"
      ],
      "metadata": {
        "id": "Rwp-1sOL4IW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('data information')\n",
        "print(Corpus.info(), '\\n')\n",
        "\n",
        "# print missing values information\n",
        "print('missing values stats')\n",
        "print(Corpus.isnull().sum(), '\\n')"
      ],
      "metadata": {
        "id": "xS5tscO44Iyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Corpus = Corpus.dropna(subset=['Cat'])\n",
        "Corpus = Corpus.dropna(subset=['Text'])\n",
        "Corpus = Corpus.drop_duplicates(subset=['Text'], keep='first')\n",
        "Corpus = Corpus.reset_index(drop=True)\n",
        "\n",
        "# print data information\n",
        "print('data information')\n",
        "print(Corpus.info(), '\\n')\n",
        "\n",
        "# print missing values information\n",
        "print('missing values stats')\n",
        "print(Corpus.isnull().sum(), '\\n')"
      ],
      "metadata": {
        "id": "ytsIFepB4Lo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqeSrWxaECb2"
      },
      "source": [
        "**Splitting the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD4PJE2bNKww"
      },
      "outputs": [],
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each post.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# The embedding dimension\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(Corpus['Text'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "X = tokenizer.texts_to_sequences(Corpus['Text'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(Corpus['Cat']).values\n",
        "print('Shape of label tensor:', Y.shape)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "id": "oOqa7aFdQhZL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Preprocessing Dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}